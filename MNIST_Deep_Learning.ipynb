{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_Deep_Learning.ipynb","provenance":[],"collapsed_sections":["AWfY1bx5XPb7"],"toc_visible":true,"mount_file_id":"1FxJF8WKdLCMX7-BwsbbacsNbfWPv9I2J","authorship_tag":"ABX9TyPcCfMhF0j2a397KFgnfgvg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3zrffDxXXAGK","colab_type":"text"},"source":["## Neural Network for MNIST Classification:\n","---\n","Author : Frissian Viales\\\n","Date : 09-13-2020"]},{"cell_type":"markdown","metadata":{"id":"AWfY1bx5XPb7","colab_type":"text"},"source":["### Setup:"]},{"cell_type":"code","metadata":{"id":"9oMVQazfW26d","colab_type":"code","colab":{}},"source":["# Import Libraries:\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D, BatchNormalization, MaxPooling2D, Flatten\n","from tensorflow.keras import Sequential\n","from keras.utils import to_categorical\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","from sklearn.datasets import fetch_openml\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfwwtdUNX8CK","colab_type":"code","colab":{}},"source":["def Fetch_Data():\n","\n","  \"\"\" A function to download the MNIST dataset from skelarn.  \"\"\"\n","\n","  # Fetch data:\n","  mnist = fetch_openml('mnist_784', version=1, cache=True)\n","\n","  # Preprocess data:\n","  mnist.target = mnist.target.astype(np.int8)\n","  reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n","  reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n","  mnist.data[:60000] = mnist.data[reorder_train]\n","  mnist.target[:60000] = mnist.target[reorder_train]\n","  mnist.data[60000:] = mnist.data[reorder_test + 60000]\n","  mnist.target[60000:] = mnist.target[reorder_test + 60000]\n","\n","  # Assign features and target variables:\n","  X, y = mnist[\"data\"], mnist[\"target\"]\n","  print('Fetched MNIST dataset successfully.')\n","  return [X,y]\n","\n","def Stratified_Split(X, y):\n","\n","  \"\"\" A function to perform a stratified train-val-test split.\"\"\" \n","\n","  # Execute train/test split:\n","  X_train, X_test, y_train, y_test = train_test_split(\n","      X, y, \n","      train_size=0.7, \n","      random_state=69,\n","      stratify=y\n","      )\n","  \n","  # Execute test/validation split:\n","  X_test, X_val, y_test, y_val = train_test_split(\n","      X_test, y_test, \n","      train_size=0.33, \n","      random_state=69,\n","      stratify=y_test\n","      )\n","  \n","  print('Splited data into Train-Validation-Test sets.')\n","  return [X_train, X_test, X_val, y_train, y_test, y_val]    \n","\n","from scipy.ndimage.interpolation import shift\n","\n","def Shift_Image(image, dx, dy):\n","\n","  \"\"\" A function to shift an image into one direction.\"\"\"\n","\n","  # Reshape:\n","  image = image.reshape((28, 28))\n","  shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n","  return shifted_image.reshape([-1])\n","\n","def Data_Augmentation(X_train, y_train):\n","\n","  \"\"\" A function to generate more training data for image classification.\n","  It shifts an image one pixel to each side. \"\"\"\n","\n","  # Initialize lists:\n","  X_train_augmented = [image for image in X_train]\n","  y_train_augmented = [label for label in y_train]\n","\n","  # Create loops to shift each image to each side:\n","  for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n","\n","    for image, label in zip(X_train, y_train):\n","\n","        X_train_augmented.append(Shift_Image(image, dx, dy))\n","        y_train_augmented.append(label)\n","\n","  # Convert back to array:\n","  X_train_augmented = np.array(X_train_augmented)\n","  y_train_augmented = np.array(y_train_augmented)\n","\n","  # Reshuffle data:\n","  shuffle_idx = np.random.permutation(len(X_train_augmented))\n","  X_train_augmented = X_train_augmented[shuffle_idx]\n","  y_train_augmented = y_train_augmented[shuffle_idx]\n","  print('Completed data augmentation successfully.')\n","\n","  return [X_train_augmented, y_train_augmented]\n","\n","\n","def Data_Preprocessing(X, y, model_type='DNN'):\n","\n","  \"\"\" A function to prepare the dataset for passing it to a NN.\"\"\" \n","\n","  # Perform data normalization: \n","  Scaler = MinMaxScaler()\n","  X_Scaled = Scaler.fit_transform(X)\n","\n","  # Perform Train-Validation-Test split:\n","  X_train, X_test, X_val, y_train, y_test, y_val = Stratified_Split(X_Scaled, y)\n","\n","  # Perform data augmentation:\n","  X_train_augmented, y_train_augmented = Data_Augmentation(X_train, y_train)\n","\n","  # Reshape data to 3D for CNN models:\n","  if model_type == 'CNN': \n","\n","    X_train_augmented = X_train_augmented.reshape((X_train_augmented.shape[0], 28, 28, 1))\n","    X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n","    X_val = X_val.reshape((X_val.shape[0], 28, 28, 1))\n","\n","  # Perform One-Hot encoding to the outputs:\n","  y_train_augmented = to_categorical(y_train_augmented)\n","  y_test = to_categorical(y_test)\n","  y_val = to_categorical(y_val)\n","\n","  print('Data preprocessing completed.\\n')\n","  return [X_train_augmented, y_train_augmented, X_val, y_val, X_test, y_test]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKzq7kzkdMFK","colab_type":"code","colab":{}},"source":["def CNN_Model():\n","\n","  # Initialize sequencial model:\n","  model = Sequential()\n","\n","  # Add hidden layers:\n","  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n","  model.add(MaxPooling2D((2, 2)))\n","  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n","  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n","  model.add(MaxPooling2D((2, 2)))\n","  model.add(Flatten())\n","  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n","  model.add(Dense(10, activation='softmax'))\n","\n","  # Compile the model:\n","  opt = SGD(lr=0.01, momentum=0.9)\n","  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","  return model\n","\n","def DNN_Model():\n","  \n","  # Initialize sequencial model:\n","  model = Sequential()\n","\n","  # Add hidden layers:\n","  model.add(Dense(784, activation='relu', input_shape=(784,)))\n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dropout(0.2))\n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dropout(0.2))\n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dropout(0.2))        \n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dropout(0.2))\n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dropout(0.2))\n","  model.add(Dense(10, activation='softmax'))\n","  \n","  # Compile the model:\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9B7WPfDeqa11","colab_type":"code","colab":{}},"source":["# evaluate a model using k-fold cross-validation\n","def Evaluate_Model(model, X_train, y_train, X_val, y_val, n_folds=5, epochs=10):\n","\n","  \"\"\" A function to perform k-fold cross validation. \"\"\" \n","\n","  # Initialize lists to store results:\n","  print('Initializing model training.\\n')\n","  scores, histories = list(), list()\n","  # Prepare cross validation:\n","  kfold = KFold(n_folds, shuffle=True, random_state=69)\n","  n = 1\n","\n","  # Enumerate splits:\n","  for train_ix, test_ix in kfold.split(X_train):\n","\n","    # Select rows for train and test:\n","    trainX, trainY, testX, testY = X_train[train_ix], y_train[train_ix], X_train[test_ix], y_train[test_ix]\n","    # Train the model:\n","    history = model.fit(trainX, trainY, epochs=epochs, validation_data=(testX, testY), verbose=1)\n","    # Evaluate the model:\n","    _, acc = model.evaluate(X_val, y_val, verbose=0)\n","    print('> %.3f' % (acc * 100.0))\n","    # Save results:\n","    scores.append(acc)\n","    histories.append(history)\n","    print(f'Completed training round {n} out of {n_folds}.')\n","    n = n + 1\n","\n","  return scores, histories"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlLMlsFbsaoD","colab_type":"code","colab":{}},"source":["# plot diagnostic learning curves\n","def Plot_Learning_Curves(histories):\n","\n","  \"\"\" PLot learning curves. \"\"\"\n","  \n","  # Set matplotlib style and plot size:\n","  plt.style.use('dark_background')\n","  plt.figure(figsize=(21,13))\n","\n","  # Plot data:\n","  for i in range(len(histories)):\n","\n","    # Plot training and validation loss:\n","    plt.subplot(2, 1, 1)\n","    plt.title('Cross Entropy Loss', fontsize=16)\n","    n = len(histories[i].history['loss'])\n","    epochs = np.arange(1, n+1, step=1.0)\n","    plt.plot(epochs, histories[i].history['loss'], color='cyan', label=f'Training_{i+1}')\n","    plt.plot(epochs, histories[i].history['val_loss'], color='#ff9900', label=f'Testing_{i+1}')\n","    plt.xticks(np.arange(1, n+1, step=1.0))\n","\n","      # Plot training and validation accuracy:\n","    plt.subplot(2, 1, 2)\n","    plt.title('Classification Accuracy', fontsize=16)\n","    n = len(histories[i].history['loss'])\n","    epochs = np.arange(1, n+1, step=1.0)  \n","    plt.plot(epochs, histories[i].history['accuracy'], color='cyan', label=f'Training_{i+1}')\n","    plt.plot(epochs, histories[i].history['val_accuracy'], color='#ff9900', label=f'Testing_{i+1}')\n","    plt.xticks(np.arange(1, n+1, step=1.0))  \n","    plt.xlabel('epochs', fontsize=12)\n","\n","  plt.legend(['Training','Test'], fontsize=12)  \n","\n","# summarize model performance\n","def Scores_BoxPlot(scores):\n","\n","  \"\"\" Plot distribution of evaliation scores per cross fold. \"\"\"\n","\n","  # Configure style and set parameters:\n","  plt.style.use('dark_background')\n","  edge_color = 'crimson'\n","  fill_color = 'lightsalmon'\n","  mean = round(np.mean(scores)*100, 2)\n","  median = round(np.median(scores)*100, 2)\n","  std = round(np.std(scores)*100, 2)\n","\n","  # Summary stats:\n","  print(f'\\nAccuracy Summary: Mean={mean}% Median={median}% Std={std}%\\n')\n","\n","  # Plot boxplot:\n","  plt.figure(figsize=(13,8))\n","  plt.title('Evaluation Scores Distirbution', fontsize=16)\n","  box = plt.boxplot(scores, patch_artist=True)\n","\n","  # Format colors:\n","  for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n","    plt.setp(box[element], color=edge_color, linewidth=2)\n","\n","  for patch in box['boxes']:\n","    patch.set(facecolor=fill_color)     \n","\n","  plt.show()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOBwJV2PleOz","colab_type":"code","colab":{}},"source":["# run the test harness for evaluating a model\n","def Run_Test_Harness(model_type='CNN', epochs=10, n_folds=5, save_model=False):\n","\n","  \"\"\" Run test \"\"\" \n","\n","  # Load dataset:\n","  X, y = Fetch_Data()\n","\n","  # Process data: \n","  X_train, y_train, X_val, y_val, X_test, y_test = Data_Preprocessing(X, y, model_type=model_type)\n","\n","  # CNN model training:\n","  if model_type == 'CNN':\n","\n","    # Initialize model:\n","    model = CNN_Model()\n","\n","    # Just fit the model and save it:\n","    if save_model:\n","\n","      model.fit(X_train, y_train, epochs=epochs, verbose=1)\n","      model.save(f'MNIST_Keras_{model_type}.h5')\n","\n","    # Just perform cross evaluation:\n","    else:\n","\n","      # Evaluation: \n","      scores, histories = Evaluate_Model(\n","          model,\n","          X_train,\n","          y_train, \n","          X_val, \n","          y_val,\n","          epochs=epochs,\n","          n_folds=n_folds\n","          )\n","      \n","      # # Plot learning curves:\n","      Plot_Learning_Curves(histories)\n","\n","      # Report estimated performance:\n","      Scores_BoxPlot(scores) \n","\n","  # DNN model training:     \n","  elif model_type == 'DNN':\n","\n","    # Initialize model:\n","    model = DNN_Model()\n","\n","    # Just fit the model and save it:\n","    if save_model:\n","\n","      model.fit(X_train, y_train, epochs=epochs, verbose=1)\n","      model.save(f'MNIST_Keras_{model_type}.h5') \n","      print(f'Saved MNIST_Keras_{model_type} successfully.')   \n","    \n","    # Just perform cross evaluation:\n","    else:\n","      scores, histories = Evaluate_Model(\n","          DNN_Model(),\n","          X_train,\n","          y_train, \n","          X_val, \n","          y_val,\n","          epochs=epochs,\n","          n_folds=n_folds\n","          )  \n","\n","      # Plot learning curves:\n","      Plot_Learning_Curves(histories)\n","\n","      # Report estimated performance:\n","      Scores_BoxPlot(scores) \n","\n","  else:\n","\n","    print('Model type does not exist.') \n","\n","  print('\\nEnd of script.')\n","  return None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8zo2mpPhIMv","colab_type":"text"},"source":["### Run Test Harness:"]},{"cell_type":"code","metadata":{"id":"IUeTWztZ_DiZ","colab_type":"code","colab":{}},"source":["Run_Test_Harness(model_type='CNN', epochs=10, n_folds=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-Xb05i1ZR8G","colab_type":"code","colab":{}},"source":["Run_Test_Harness(model_type='DNN', epochs=10, n_folds=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYk0GCISnEJu","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}